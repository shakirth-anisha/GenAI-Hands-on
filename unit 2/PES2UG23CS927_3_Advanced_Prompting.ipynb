{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2fe75baa",
      "metadata": {
        "id": "2fe75baa"
      },
      "source": [
        "---\n",
        "\n",
        "Name : Shakirth Anisha\n",
        "\n",
        "SRN: PES2UG23CS927\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e80a8ddf",
      "metadata": {
        "id": "e80a8ddf"
      },
      "source": [
        "# Unit 2 - Part 3a: Chain of Thought (CoT)\n",
        "\n",
        "## 1. Introduction: The Inner Monologue\n",
        "\n",
        "Standard LLMs try to jump straight to the answer. For complex problems (math, logic), this often fails.\n",
        "\n",
        "**Chain of Thought (CoT)** forces the model to \"think out loud\" before answering.\n",
        "\n",
        "### Why use a \"Dumb\" Model?\n",
        "For this unit, we will use **Llama3.1-8b** (via Groq). It is a smaller, faster model.\n",
        "Why? Because huge models (like Gemini Pro or GPT-4) are often *too smart*—they solve logic riddles instantly without thinking.\n",
        "\n",
        "To really see the power of Prompt Engineering, we need a model that **needs help**.\n",
        "\n",
        "### Visualizing the Process (Flowchart)\n",
        "```mermaid\n",
        "graph TD\n",
        "    Input[Question: 5+5*2?]\n",
        "    Input -->|Standard| Wrong[Answer: 20 (Wrong)]\n",
        "    Input -->|CoT| Step1[Step 1: 5*2=10]\n",
        "    Step1 --> Step2[Step 2: 5+10=15]\n",
        "    Step2 --> Correct[Answer: 15 (Correct)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a41e5ba",
      "metadata": {
        "id": "0a41e5ba"
      },
      "source": [
        "## 2. Concept: Latent Reasoning\n",
        "\n",
        "Why does this work?\n",
        "Because LLMs are \"Next Token Predictors\".\n",
        "- If you force it to answer immediately, it must predict the digits `1` and `5` immediately.\n",
        "- If you let it \"think\", it generates intermediate tokens (`5`, `*`, `2`, `=`, `1`, `0`).\n",
        "- The model then **ATTENDS** to these new tokens to compute the final answer.\n",
        "\n",
        "**Writing is Thinking.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ba92b198",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba92b198",
        "outputId": "de5498f2-604b-4b96-bec9-00a1cb2f99ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m133.1/137.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Groq API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3088780",
      "metadata": {
        "id": "e3088780"
      },
      "source": [
        "## 3. The Experiment: A Tricky Math Problem\n",
        "\n",
        "Let's try a problem that requires multi-step logic.\n",
        "\n",
        "**Problem:**\n",
        "\"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a70d3b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a70d3b7",
        "outputId": "11c0984a-e9af-4f5a-a337-52072c823161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STANDARD (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Anisha has now, we need to add the initial number of tennis balls she had (5) to the number of tennis balls she bought (2 cans * 3 tennis balls per can).\n",
            "\n",
            "2 cans * 3 tennis balls per can = 6 tennis balls\n",
            "\n",
            "Now, let's add the initial number of tennis balls (5) to the number of tennis balls she bought (6):\n",
            "\n",
            "5 + 6 = 11\n",
            "\n",
            "Anisha now has 11 tennis balls.\n"
          ]
        }
      ],
      "source": [
        "question = \"Anisha has 5 tennis balls. She buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does she have now?\"\n",
        "\n",
        "# 1. Standard Prompt (Direct Answer)\n",
        "prompt_standard = f\"Answer this question: {question}\"\n",
        "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_standard).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b696ba6",
      "metadata": {
        "id": "5b696ba6"
      },
      "source": [
        "### Critique\n",
        "Smaller models often latch onto the visible numbers (5 and 2) and simply add them (7), ignoring the multiplication step implied by \"cans\".\n",
        "\n",
        "Let's force it to think."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3dd65b0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dd65b0a",
        "outputId": "59df896d-e4a7-4170-cf08-36a79f6cc451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chain of Thought (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Anisha has now, we need to follow these steps:\n",
            "\n",
            "1. Anisha already has 5 tennis balls.\n",
            "2. She buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
            "3. To find out how many tennis balls she got from the cans, we multiply the number of cans by the number of tennis balls in each can: 2 cans * 3 tennis balls/can = 6 tennis balls.\n",
            "4. Now, we add the tennis balls she already had to the tennis balls she got from the cans: 5 tennis balls + 6 tennis balls = 11 tennis balls.\n",
            "\n",
            "So, Anisha now has 11 tennis balls.\n"
          ]
        }
      ],
      "source": [
        "# 2. CoT Prompt (Magic Phrase)\n",
        "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
        "\n",
        "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_cot).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd205672",
      "metadata": {
        "id": "bd205672"
      },
      "source": [
        "## 4. Analysis\n",
        "\n",
        "Look at the output. By explicitly breaking it down:\n",
        "1.  \"Roger starts with 5.\"\n",
        "2.  \"2 cans * 3 balls = 6 balls.\"\n",
        "3.  \"5 + 6 = 11.\"\n",
        "\n",
        "The model effectively \"debugs\" its own logic by generating the intermediate steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ee779f",
      "metadata": {
        "id": "22ee779f"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d1fa7c",
      "metadata": {
        "id": "11d1fa7c"
      },
      "source": [
        "# Unit 2 - Part 3b: Tree of Thoughts (ToT) & Graph of Thoughts (GoT)\n",
        "\n",
        "## 1. Introduction: Beyond A -> B\n",
        "\n",
        "CoT is linear. But complex reasoning is often nonlinear. We need to explore branches (ToT) or even combine ideas (GoT).\n",
        "\n",
        "We continue using **Llama3.1-8b via Groq** to show how structure improves performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4371aa3d",
      "metadata": {
        "id": "4371aa3d"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a348d6",
      "metadata": {
        "id": "03a348d6"
      },
      "source": [
        "## 2. Tree of Thoughts (ToT)\n",
        "\n",
        "ToT explores multiple branches before making a decision.\n",
        "**Analogy:** A chess player considering 3 possible moves before playing one.\n",
        "\n",
        "### Implementation\n",
        "We will generate 3 distinct solutions for a problem and then use a \"Judge\" to pick the best one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1ea2d4c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea2d4c7",
        "outputId": "927c1fa8-5e7d-4dd4-a1bc-f0717c0b8363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tree of Thoughts (ToT) Result ---\n",
            "Based on the three proposed solutions, I would recommend **Solution 3: Create a Personal Project that Demonstrates Your Expertise and Passion** as the most sustainable and fast way to get an internship in FAANG+ companies.\n",
            "\n",
            "Here's why:\n",
            "\n",
            "1. **Control and Autonomy**: By creating a personal project, you have complete control over the scope, timeline, and outcome. You can work at your own pace and make decisions about the direction of the project.\n",
            "2. **Flexibility**: This approach allows you to work on a project that aligns with your interests and skills, which means you can be more creative and passionate about the work. You can also adjust the project scope or timeline as needed.\n",
            "3. **Cost-Effective**: Creating a personal project requires minimal investment, as you likely have the necessary resources and skills already. You can also use free or low-cost tools and platforms to host and showcase your project.\n",
            "4. **Scalability**: A personal project can be scaled up or down depending on your goals and resources. You can start small and gradually build out the project as you gain more experience and skills.\n",
            "5. **Long-Term Value**: A well-executed personal project can have long-term value, even after the internship or job application process is complete. You can continue to work on the project, refine it, and use it as a portfolio piece or case study.\n",
            "6. **Networking Opportunities**: A personal project can provide a unique conversation starter and help you connect with potential employers, colleagues, and mentors in the industry.\n",
            "7. **Competitive Advantage**: By creating a personal project that demonstrates your expertise and passion, you can differentiate yourself from other candidates and showcase your skills and commitment to the industry.\n",
            "\n",
            "In contrast, while hackathons and coding challenges (Solution 1) and online hackathons (Solution 2) can be valuable experiences, they may not provide the same level of long-term value or control as a personal project. Additionally, participating in these events may require significant time and resources, which can be a barrier for some students.\n",
            "\n",
            "To implement Solution 3 effectively, I recommend the following:\n",
            "\n",
            "* Identify a domain or problem area that aligns with your interests and skills.\n",
            "* Define a clear problem statement and scope for the project.\n",
            "* Develop a prototype or proof-of-concept that addresses the problem statement.\n",
            "* Create a professional website or blog to showcase the project.\n",
            "* Network and engage with potential employers, colleagues, and mentors in the industry.\n",
            "* Continuously work on and refine the project to demonstrate your commitment and passion.\n",
            "\n",
            "By following this approach, you can create a personal project that showcases your expertise and passion, increases your chances of getting an internship at FAANG+ companies, and provides long-term value and networking opportunities.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "problem = \"How can I get an internship in FAANG+?\"\n",
        "\n",
        "# Step 1: The Branch Generator\n",
        "prompt_branch = ChatPromptTemplate.from_template(\n",
        "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
        ")\n",
        "\n",
        "branches = RunnableParallel(\n",
        "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
        "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
        "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# Step 2: The Judge\n",
        "prompt_judge = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three proposed solutions for: '{problem}'\n",
        "\n",
        "    1: {sol1}\n",
        "    2: {sol2}\n",
        "    3: {sol3}\n",
        "\n",
        "    Act as a Placement Coordinator for a prestigeous college. Pick the most sustainable and fast one and explain why.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Chain: Input -> Branches -> Judge -> Output\n",
        "tot_chain = (\n",
        "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
        "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]})\n",
        "    | prompt_judge\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
        "print(tot_chain.invoke(problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38579cab",
      "metadata": {
        "id": "38579cab"
      },
      "source": [
        "## 3. Graph of Thoughts (GoT)\n",
        "\n",
        "You asked: **\"Where is Graph of Thoughts?\"**\n",
        "\n",
        "GoT is more complex. It's a network. Information can split, process specific parts, and then **AGGREGATE** back together.\n",
        "\n",
        "### The Workflow (Writer's Room)\n",
        "1.  **Split:** Generate 3 independent story plots (Sci-Fi, Fantasy, Mystery).\n",
        "2.  **Aggregate:** The model reads all 3 and creates a \"Master Plot\" that combines the best elements of each.\n",
        "3.  **Refine:** Polish the Master Plot.\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "   Start(Concept) --> A[Draft 1]\n",
        "   Start --> B[Draft 2]\n",
        "   Start --> C[Draft 3]\n",
        "   A & B & C --> Mixer[Aggregator]\n",
        "   Mixer --> Final[Final Story]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "894940b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "894940b5",
        "outputId": "817333bc-f9e8-417b-c903-353062f2a2b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Graph of Thoughts (GoT) Result ---\n",
            "In \"Echoes of Eternity,\" a brilliant and rebellious scientist, Maya, discovers a way to manipulate the fabric of time in a dystopian future where time travel is strictly regulated by the tyrannical government. As she delves deeper into the mysteries of the timestream, Maya finds herself inexplicably drawn to a charismatic figure from her past, a man named Elijah who was killed in a tragic accident. However, their rekindled romance is threatened by a malevolent entity that begins to manipulate the timeline, creating gruesome temporal loops and alternate realities that blur the lines between past and present. As Maya fights to save the world from destruction and her own sanity from unraveling, she must navigate the treacherous landscape of her own heart, confronting the darkest corners of her own psyche and the true cost of her all-consuming love for Elijah.\n"
          ]
        }
      ],
      "source": [
        "# 1. The Generator (Divergence)\n",
        "prompt_draft = ChatPromptTemplate.from_template(\n",
        "    \"Write a 1 sentence movie plot about: {topic}. Genre: {genre}.\"\n",
        ")\n",
        "\n",
        "drafts = RunnableParallel(\n",
        "    draft_scifi=prompt_draft.partial(genre=\"Dystopian\") | llm | StrOutputParser(),\n",
        "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
        "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# 2. The Aggregator (Convergence)\n",
        "prompt_combine = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three movie ideas for the topic '{topic}':\n",
        "    1. Dystopian: {draft_scifi}\n",
        "    2. Romance: {draft_romance}\n",
        "    3. Horror: {draft_horror}\n",
        "\n",
        "    Your task: Create a new Mega Movie that combines the TECHNOLOGY of Dystopian, the PASSION of Romance, and the FEAR of Horror.\n",
        "    Write one paragraph.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 3. The Chain\n",
        "got_chain = (\n",
        "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
        "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]})\n",
        "    | prompt_combine\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
        "print(got_chain.invoke(\"Time Travel\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455cb724",
      "metadata": {
        "id": "455cb724"
      },
      "source": [
        "## 4. Summary & Comparison Table\n",
        "\n",
        "| Method | Structure | Best For... | Cost/Latency |\n",
        "|--------|-----------|-------------|--------------|\n",
        "| **Simple Prompt** | Input -> Output | Simple facts, summaries | ⭐ Low |\n",
        "| **CoT (Chain)** | Input -> Steps -> Output | Math, Logic, Debugging | ⭐⭐ Med |\n",
        "| **ToT (Tree)** | Input -> 3x Branches -> Select -> Output | Strategic decisions, Brainstorming | ⭐⭐⭐ High |\n",
        "| **GoT (Graph)** | Input -> Branch -> Mix/Aggregate -> Output | Creative Writing, Research Synthesis | ⭐⭐⭐⭐ V. High |\n",
        "\n",
        "**Recommendation:** Start with CoT. Only use ToT/GoT if CoT fails."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}